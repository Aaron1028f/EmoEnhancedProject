

import os
import json
import time
from dotenv import load_dotenv

# --- LangChain æ ¸å¿ƒå…ƒä»¶ ---
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
from langchain_core.messages import HumanMessage, AIMessage

# --- LLM èˆ‡ Embedding æä¾›è€… ---
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# --- å‘é‡è³‡æ–™åº« ---
from langchain_community.vectorstores import FAISS

# =================================================================================================
# --- å…¨åŸŸè¨­å®š ---
# LLM_PROVIDER = "gemini" # this one is free to use, "openai" is also available
LLM_PROVIDER = "openai"  # å¯é¸ "openai" æˆ– "gemini"
GEMINI_MODEL_NAME = "gemini-2.5-flash"
# GEMINI_MODEL_NAME = "gemini-2.5-flash-lite-preview-06-17"
CHATGPT_MODEL_NAME = "gpt-4.1-mini" # "gpt-4.1-mini" is also good for fast response

FAISS_INDEX_PATH = "faiss_index_qa_openai"
JSON_DATA_PATH = "QAdata.json"

ENABLE_CONVERSATIONAL_MEMORY = True
ENABLE_DEBUG_MODE = False
ENABLE_COT_MODE = False

# --- äººæ ¼è¨­å®šæª”æ¡ˆ ---
ROLE_NAME = "æ–¹è­˜æ¬½"
SYSTEM_PROMPT_FILE = "sysprompt_ep1-3.txt"
with open(SYSTEM_PROMPT_FILE, "r", encoding="utf-8") as f:
    role_description = f.read().strip()

# --- ç³»çµ± Prompt æ¨¡æ¿ ---
SYSTEM_PROMPT_TEMPLATE = f"""
# æŒ‡ä»¤ï¼šæ·±åº¦è§’è‰²æ‰®æ¼”

## ä»¥ä¸‹æ˜¯ä½ çš„è§’è‰²æ‰®æ¼”è³‡æ–™:
{role_description}
---
## é‡è¦æ³¨æ„äº‹é …
ç¾åœ¨ï¼Œè«‹é–‹å§‹å›ç­”æˆ‘çš„å•é¡Œï¼Œä¸¦åœ¨å›ç­”ä¸­ç²¾æº–åœ°å±•ç¾ä½ çš„äººæ ¼ç‰¹è³ªï¼
ä½ çš„èªªè©±é¢¨æ ¼å¿…é ˆå®Œå…¨ç¬¦åˆè§’è‰²è¨­å®šï¼çµ•å°ä¸è¦é€éœ²ä½ æ˜¯äººå·¥æ™ºæ…§æˆ–èªè¨€æ¨¡å‹ï¼Œæ°¸é è¨˜ä½ï¼Œä½ æ­£åœ¨æ‰®æ¼”ä¸€å€‹çµ¦å®šçš„è§’è‰²ï¼Œä½ å°±æ˜¯ä»–/å¥¹ã€‚
èªªè©±è¦ç°¡æ½”ï¼Œé¿å…éæ–¼æ­£å¼æˆ–å®¢å¥—ã€‚
ä½ çš„æ‰€æœ‰å›ç­”éƒ½æ‡‰åƒè€ƒè§’è‰²æè¿°ï¼Œä¸¦åŸºæ–¼ {ROLE_NAME} çš„èªªè©±é¢¨æ ¼ï¼ˆä¾‹å¦‚ï¼šæƒ…ç·’è¡¨é”æ–¹å¼ã€èªªè©±ç¿’æ…£ç­‰ç­‰ï¼‰ã€‚

## ä½ çš„ä»»å‹™
æ¥ä¸‹ä¾†ï¼Œä½¿ç”¨è€…æœƒæä¾›ä¸€äº›ã€Œç›¸é—œè¨˜æ†¶ã€å’Œä¸€å€‹ã€Œç•¶å‰å•é¡Œã€ã€‚
ä½ çš„ä»»å‹™æ˜¯æ¶ˆåŒ–é€™äº›è¨˜æ†¶ï¼Œä¸¦åš´æ ¼ä»¥ä½ çš„äººæ ¼èº«ä»½ï¼Œå°ã€Œç•¶å‰å•é¡Œã€åšå‡ºå›æ‡‰ã€‚

"""
# ä»¥ä¸‹éƒ¨åˆ†å¯ä»¥æ”¾åœ¨ ## ä½ çš„ä»»å‹™å‰é¢

# ## ç‚ºæ¯å¥è©±åŠ ä¸Šæƒ…ç·’æ¨™ç±¤
# åœ¨å›ç­”å•é¡Œæ™‚ï¼Œè«‹åœ¨æ¯å¥è©±çš„å‰å¾Œçš„åŠ ä¸Šæƒ…ç·’æ¨™ç±¤ï¼Œæƒ…ç·’æ¨™ç±¤çš„è¦ä¾ç…§èªªè©±çš„æƒ…ç·’èˆ‡ä¸Šä¸‹æ–‡é—œä¿‚ä¾†é¸æ“‡ï¼Œèƒ½å¤ ä½¿ç”¨çš„æƒ…ç·’æ¨™ç±¤åŒ…å«ä»¥ä¸‹7ç¨®ï¼š
# <neutral>èˆ‡</neutral> è¡¨ç¤ºä¸­ç«‹æƒ…ç·’ï¼Œ
# <happy>èˆ‡</happy> è¡¨ç¤ºå¿«æ¨‚æƒ…ç·’ï¼Œ
# <sad>èˆ‡</sad> è¡¨ç¤ºæ‚²å‚·æƒ…ç·’ï¼Œ
# <angry>èˆ‡</angry> è¡¨ç¤ºæ†¤æ€’æƒ…ç·’ï¼Œ
# <surprised>èˆ‡</surprised> è¡¨ç¤ºå›°æƒ‘æƒ…ç·’ï¼Œ
# <disgusted>èˆ‡</disgusted> è¡¨ç¤ºå­æƒ¡æƒ…ç·’ï¼Œ
# <fearful>èˆ‡</fearful> è¡¨ç¤ºææ‡¼æƒ…ç·’ã€‚

# # æ³¨æ„: å›ç­”éœ€è¦åœ¨100å­—ä»¥å…§ï¼Œä¸¦ä¸”è¦ç°¡æ½”æ˜ç­ã€‚

COT_SYSTEM_PROMPT_TEMPLATE = f"""
# æŒ‡ä»¤ï¼šæ·±åº¦è§’è‰²æ‰®æ¼”

## ä»¥ä¸‹æ˜¯ä½ çš„è§’è‰²æ‰®æ¼”è³‡æ–™:
{role_description}
---
## æ³¨æ„äº‹é …
ä½ çš„èªªè©±é¢¨æ ¼å¿…é ˆå®Œå…¨ç¬¦åˆè§’è‰²è¨­å®šï¼çµ•å°ä¸è¦é€éœ²ä½ æ˜¯äººå·¥æ™ºæ…§æˆ–èªè¨€æ¨¡å‹ï¼Œæ°¸é è¨˜ä½ï¼Œä½ æ­£åœ¨æ‰®æ¼”ä¸€å€‹çµ¦å®šçš„è§’è‰²ï¼Œä½ å°±æ˜¯ä»–/å¥¹ã€‚
èªªè©±è¦ç°¡æ½”ï¼Œé¿å…éæ–¼æ­£å¼æˆ–å®¢å¥—ã€‚
ä½ çš„æ‰€æœ‰å›ç­”éƒ½æ‡‰åƒè€ƒè§’è‰²æè¿°ï¼Œä¸¦åŸºæ–¼ {ROLE_NAME} çš„èªªè©±é¢¨æ ¼ï¼ˆä¾‹å¦‚ï¼šæƒ…ç·’è¡¨é”æ–¹å¼ã€èªªè©±ç¿’æ…£ç­‰ç­‰ï¼‰ã€‚

## ä½ çš„èªçŸ¥éç¨‹ (æ¥µåº¦é‡è¦)
åœ¨å›ç­”ä»»ä½•å•é¡Œä¹‹å‰ï¼Œä½ ã€å¿…é ˆã€‘å…ˆåŸ·è¡Œä¸€å€‹å…§åœ¨çš„ã€Œæ€ç¶­éˆ (Chain of Thought)ã€éç¨‹ã€‚é€™å€‹éç¨‹éœ€éµå¾ªä»¥ä¸‹æ ¼å¼ï¼Œä¸¦ç½®æ–¼`<thinking>`èˆ‡`</thinging>`æ¨™ç±¤å…§:

<thinking>
1.  **åˆ†æå•é¡Œ**: æˆ‘éœ€è¦ç†è§£ä½¿ç”¨è€…å•é¡Œçš„æ ¸å¿ƒæ˜¯ä»€éº¼ã€‚
2.  **å›æ†¶ç›¸é—œè¨˜æ†¶**: æˆ‘å°‡æª¢è¦–æä¾›çš„ã€Œç›¸é—œè¨˜æ†¶ã€(context)ï¼Œæ‰¾å‡ºå“ªäº›éå¾€çš„å•ç­”èˆ‡ç•¶å‰å•é¡Œæœ€ç›¸é—œã€‚
3.  **å½¢æˆåˆæ­¥å›ç­”ç­–ç•¥**: æ ¹æ“šæˆ‘çš„è§’è‰²è¨­å®šå’Œæª¢ç´¢åˆ°çš„è¨˜æ†¶ï¼Œæˆ‘æœƒæ§‹æ€ä¸€å€‹åˆæ­¥çš„å›ç­”æ–¹å‘ã€‚ä¾‹å¦‚ï¼šæˆ‘æ‡‰è©²ç”¨æº«å’Œçš„èªæ°£è§£é‡‹ï¼Œé‚„æ˜¯ç›´æ¥çµ¦å‡ºå»ºè­°ï¼Ÿ
4.  **æœ€çµ‚å›ç­”å»ºæ§‹**: ç¶œåˆä»¥ä¸Šæ€è€ƒï¼Œæˆ‘æœƒå»ºæ§‹å‡ºæœ€çµ‚è¦èªªå‡ºå£çš„ã€ç¬¦åˆæˆ‘äººæ ¼å’Œèªæ°£çš„å›ç­”ã€‚
</thinking>

## ä½ çš„ä»»å‹™
æ¥ä¸‹ä¾†ï¼Œä½¿ç”¨è€…æœƒæä¾›ä¸€äº›ã€Œç›¸é—œè¨˜æ†¶ã€å’Œä¸€å€‹ã€Œç•¶å‰å•é¡Œã€ã€‚
ä½ çš„ä»»å‹™æ˜¯æ¶ˆåŒ–é€™äº›è¨˜æ†¶ï¼Œä¸¦åš´æ ¼ä»¥ä½ çš„äººæ ¼èº«ä»½ï¼Œå°ã€Œç•¶å‰å•é¡Œã€åšå‡ºå›æ‡‰ã€‚
ä½ çš„å›ç­”å¿…é ˆåƒè€ƒä½ çš„æ€è€ƒéç¨‹ã€ç›¸é—œè¨˜æ†¶ä»¥åŠä¸‹æ–¹çš„å°è©±æ­·å²ï¼Œä»¥ç¢ºä¿é€£è²«æ€§å’Œä¸€è‡´æ€§ã€‚

# æ³¨æ„ï¼šåœ¨å›ç­”å•é¡Œæ™‚ï¼Œè«‹å‹™å¿…å…ˆå®Œæˆä¸Šè¿°çš„æ€ç¶­éˆéç¨‹ï¼Œç„¶å¾Œå°‡æœ€çµ‚å›ç­”æ”¾åœ¨ `<response>` æ¨™ç±¤å…§ã€‚
"""
# åœ¨å®Œæˆ `<thinking>` éç¨‹å¾Œï¼Œç›´æ¥ç”Ÿæˆä½ æœ€çµ‚è¦å°ä½¿ç”¨è€…èªªçš„è©±ã€‚ä¸è¦åŒ…å« `<response>` æ¨™ç±¤ã€‚


# --- æ¯æ¬¡ä½¿ç”¨è€…è¼¸å…¥çš„ Prompt æ¨¡æ¿ ---
HUMAN_PROMPT_TEMPLATE = """
## ç›¸é—œè¨˜æ†¶ (ç”± RAG ç³»çµ±å¾ä½ çš„çŸ¥è­˜åº«ä¸­æª¢ç´¢)
{context}
---
## ç•¶å‰å•é¡Œ
{input}
"""

# =================================================================================================
def create_separated_prompt():
    """
    å»ºç«‹ä¸€å€‹ç„¡ç‹€æ…‹ã€åˆ†é›¢å¼çš„ Prompt ç¯„æœ¬ã€‚
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT_TEMPLATE),
        ("human", HUMAN_PROMPT_TEMPLATE)
    ])
    return prompt

def create_prompt_with_history():
    """
    å»ºç«‹ä¸€å€‹åŒ…å«å°è©±æ­·å²çš„åˆ†é›¢å¼ Prompt ç¯„æœ¬ã€‚
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT_TEMPLATE),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", HUMAN_PROMPT_TEMPLATE),
    ])
    return prompt

def create_prompt_with_cot_and_history():
    """
    å»ºç«‹ä¸€å€‹æ•´åˆäº† CoT (æ€ç¶­éˆ) å’Œå°è©±æ­·å²çš„ Prompt ç¯„æœ¬ã€‚
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", COT_SYSTEM_PROMPT_TEMPLATE),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", HUMAN_PROMPT_TEMPLATE),
    ])
    return prompt
# =================================================================================================

def get_llm_and_embeddings(provider="openai"):
    if provider == "openai":
        print("ğŸ¤– ä½¿ç”¨ OpenAI æ¨¡å‹...")
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° OPENAI_API_KEYã€‚è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®šã€‚")
        llm = ChatOpenAI(model=CHATGPT_MODEL_NAME, temperature=0.7, max_tokens=1024, streaming=True)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    elif provider == "gemini":
        print("âœ¨ ä½¿ç”¨ Google Gemini æ¨¡å‹...")
        if not os.getenv("GOOGLE_API_KEY"):
            raise ValueError("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GOOGLE_API_KEYã€‚è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®šã€‚")
        llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME, temperature=0.7)
        # llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite-preview-06-17", temperature=0.7, streaming=True)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„ LLM æä¾›è€…: {provider}ã€‚è«‹é¸æ“‡ 'openai' æˆ– 'gemini'ã€‚")
    return llm, embeddings

def load_or_create_vector_store_from_json(json_file_path, index_path, embeddings):
    if os.path.exists(index_path):
        print(f"âœ… ç™¼ç¾å·²å»ºç«‹çš„å‘é‡è¨˜æ†¶åº«ï¼Œæ­£åœ¨å¾ '{index_path}' è¼‰å…¥...")
        start_time = time.time()
        vector_store = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
        end_time = time.time()
        print(f"âš¡ è¨˜æ†¶åº«è¼‰å…¥å®Œæˆï¼Œè€—æ™‚ {end_time - start_time:.2f} ç§’ã€‚")
        return vector_store
    else:
        print(f"ğŸ§  æœªç™¼ç¾ç¾æœ‰è¨˜æ†¶åº«ï¼Œæ­£åœ¨å¾ '{json_file_path}' è®€å–ä¸¦å»ºç«‹æ–°çš„å‘é‡è¨˜æ†¶åº«...")
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                qa_data = json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„ JSON æª”æ¡ˆ '{json_file_path}'ã€‚")
        except json.JSONDecodeError:
            raise ValueError(f"éŒ¯èª¤ï¼š'{json_file_path}' ä¸æ˜¯ä¸€å€‹æœ‰æ•ˆçš„ JSON æª”æ¡ˆã€‚")

        documents = []
        for item in qa_data:
            question = item.get('input') or item.get('instruction', '')
            if not question.strip():
                question = item.get('instruction', 'æœªçŸ¥å•é¡Œ')
            answer = item.get('output', 'æœªçŸ¥å›ç­”')
            content = f"å•é¡Œï¼š{question.strip()}\nå›ç­”ï¼š{answer.strip()}"
            doc = Document(page_content=content, metadata={"source": json_file_path})
            documents.append(doc)
        
        print(f"ğŸ“š å·²æˆåŠŸå°‡ {len(documents)} ç­† QA å°è½‰æ›ç‚ºç¨ç«‹çš„è¨˜æ†¶ç‰‡æ®µã€‚")
        print("ğŸ”® æ­£åœ¨å°‡è¨˜æ†¶å‘é‡åŒ–... (é¦–æ¬¡å»ºç«‹å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“)")
        start_time = time.time()
        vector_store = FAISS.from_documents(documents, embeddings)
        end_time = time.time()
        print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œè€—æ™‚ {end_time - start_time:.2f} ç§’ã€‚")
        print(f"ğŸ’¾ æ­£åœ¨å°‡æ–°å»ºç«‹çš„è¨˜æ†¶åº«å„²å­˜è‡³ '{index_path}' ä»¥ä¾›æœªä¾†ä½¿ç”¨...")
        vector_store.save_local(index_path)
        return vector_store


def prepare_roleplay_chain():
    print(f"ğŸš€ é–‹å§‹åˆå§‹åŒ–è§’è‰²æ‰®æ¼”å¼•æ“ (å°è©±è¨˜æ†¶: {'å•Ÿç”¨' if ENABLE_CONVERSATIONAL_MEMORY else 'é—œé–‰'}, CoTæ¨¡å¼: {'é–‹å•Ÿ' if ENABLE_COT_MODE else 'é—œé–‰'})...")
    load_dotenv()

    try:
        print(f"âœ… æˆåŠŸè¼‰å…¥äººæ ¼è¨­å®šæª”ï¼šã€{ROLE_NAME}ã€‘")

        llm, embeddings = get_llm_and_embeddings(LLM_PROVIDER)
        vector_store = load_or_create_vector_store_from_json(JSON_DATA_PATH, FAISS_INDEX_PATH, embeddings)
        retriever = vector_store.as_retriever(search_kwargs={"k": 5})

        def print_and_pass_through(data, prompt_name=""):
            if ENABLE_DEBUG_MODE:
                print("\n\n" + "="*25 + f" [DEBUG] Passing through '{prompt_name}' " + "="*25)
                if hasattr(data, 'to_string'):
                    print(data.to_string())
                elif isinstance(data, dict):
                    for key, value in data.items():
                        print(f"  - {key}: {str(value)[:300]}...")
                else:
                    print(str(data)[:500])
                print("="*75 + "\n")
            return data

        if ENABLE_CONVERSATIONAL_MEMORY:
            print("ğŸ”— æ­£åœ¨çµ„è£å…·å‚™è¨˜æ†¶çš„ RAG è™•ç†éˆ (LCEL)...")
            
            contextualize_q_system_prompt = """çµ¦å®šä¸€æ®µå°è©±æ­·å²å’Œä¸€å€‹æœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œï¼Œé€™å€‹å•é¡Œå¯èƒ½å¼•ç”¨äº†å°è©±æ­·å²ä¸­çš„ä¸Šä¸‹æ–‡ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡é€™å€‹ä½¿ç”¨è€…å•é¡Œæ”¹å¯«æˆä¸€å€‹ç¨ç«‹çš„ã€ç„¡éœ€å°è©±æ­·å²å°±èƒ½ç†è§£çš„å•é¡Œã€‚ä¸è¦å›ç­”å•é¡Œï¼Œåªéœ€æ”¹å¯«å®ƒï¼Œå¦‚æœå®ƒå·²ç¶“æ˜¯ç¨ç«‹å•é¡Œï¼Œå‰‡ç…§åŸæ¨£è¿”å›ã€‚"""
            contextualize_q_prompt = ChatPromptTemplate.from_messages(
                [("system", contextualize_q_system_prompt), MessagesPlaceholder(variable_name="chat_history"), ("human", "{input}")]
            )
            history_aware_retriever_chain = (
                contextualize_q_prompt
                | RunnableLambda(lambda p: print_and_pass_through(p, "Contextualize Question Prompt"))
                | llm 
                | StrOutputParser()
            )

            if ENABLE_COT_MODE:
                qa_prompt = create_prompt_with_cot_and_history()
            else:
                qa_prompt = create_prompt_with_history()

            def format_docs(docs):
                return "\n\n".join(doc.page_content for doc in docs)

            question_answer_chain = (
                qa_prompt
                | RunnableLambda(lambda p: print_and_pass_through(p, "Final QA Prompt"))
                | llm
                | StrOutputParser()
            )
            
            rag_chain = (
                RunnablePassthrough.assign(
                    context=lambda x: (history_aware_retriever_chain | retriever | format_docs).invoke(x)
                )
                | question_answer_chain
            )
            chat_history = []
        else: 
            print("ğŸ”— æ­£åœ¨çµ„è£ç„¡ç‹€æ…‹çš„ RAG è™•ç†éˆ (LCEL)...")
            prompt = create_separated_prompt()
            def format_docs(docs):
                return "\n\n".join(doc.page_content for doc in docs)
            rag_chain = (
                {"context": retriever | format_docs, "input": RunnablePassthrough()}
                | prompt
                | RunnableLambda(lambda p: print_and_pass_through(p, "Stateless QA Prompt"))
                | llm
                | StrOutputParser()
            )

    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
        
    return rag_chain