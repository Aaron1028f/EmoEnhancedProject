import os
import json
import time
from dotenv import load_dotenv
from typing import AsyncGenerator, Dict, List

# --- LangChain æ ¸å¿ƒå…ƒä»¶ ---
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage

# --- LLM èˆ‡ Embedding æä¾›è€… ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# --- å‘é‡è³‡æ–™åº« ---
from langchain_community.vectorstores import FAISS

# --- å°ˆæ¡ˆæ¨¡çµ„ ---
from config import (
    LLM_PROVIDER, GEMINI_MODEL_NAME, CHATGPT_MODEL_NAME,
    FAISS_INDEX_PATH, JSON_DATA_PATH, ENABLE_CONVERSATIONAL_MEMORY,
    ENABLE_DEBUG_MODE, ENABLE_COT_MODE, ROLE_NAME, SYSTEM_PROMPT_FILE,
    SYSTEM_PROMPT_TEMPLATE, COT_SYSTEM_PROMPT_TEMPLATE, HUMAN_PROMPT_TEMPLATE
)

class RolePlayRAG:
    """
    ä¸€å€‹å°è£äº† RAG èŠå¤©æ©Ÿå™¨äººæ‰€æœ‰é‚è¼¯çš„é¡žåˆ¥ã€‚
    """
    def __init__(self):
        print("ðŸš€ é–‹å§‹åˆå§‹åŒ–è§’è‰²æ‰®æ¼”å¼•æ“Ž...")
        load_dotenv()
        self._load_role_description()
        self.llm, self.embeddings = self._get_llm_and_embeddings()
        self.vector_store = self._load_or_create_vector_store()
        self.retriever = self.vector_store.as_retriever(search_kwargs={"k": 5})
        self.rag_chain = self._create_rag_chain()
        self.chat_histories: Dict[str, List[HumanMessage | AIMessage]] = {}
        print(f"ðŸŽ­ è§’è‰²æ‰®æ¼”ç³»çµ±å·²å°±ç·’ï¼åŒ–èº«ï¼šã€{ROLE_NAME}ã€‘")
        print(f"   - ç•¶å‰ä½¿ç”¨æ¨¡åž‹: {LLM_PROVIDER.upper()}")
        print(f"   - å°è©±è¨˜æ†¶æ¨¡å¼: {'å•Ÿç”¨' if ENABLE_CONVERSATIONAL_MEMORY else 'é—œé–‰'}")
        print(f"   - CoT æ¨¡å¼: {'é–‹å•Ÿ' if ENABLE_COT_MODE and ENABLE_CONVERSATIONAL_MEMORY else 'é—œé–‰'}")


    def _load_role_description(self):
        try:
            with open(SYSTEM_PROMPT_FILE, "r", encoding="utf-8") as f:
                role_description = f.read().strip()
            self.system_prompt = SYSTEM_PROMPT_TEMPLATE.format(role_description=role_description, ROLE_NAME=ROLE_NAME)
            self.cot_system_prompt = COT_SYSTEM_PROMPT_TEMPLATE.format(role_description=role_description, ROLE_NAME=ROLE_NAME)
            print(f"âœ… æˆåŠŸè¼‰å…¥äººæ ¼è¨­å®šæª”ï¼šã€{ROLE_NAME}ã€‘")
        except FileNotFoundError:
            raise RuntimeError(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°äººæ ¼è¨­å®šæª” '{SYSTEM_PROMPT_FILE}'ã€‚")

    def _get_llm_and_embeddings(self):
        if LLM_PROVIDER == "openai":
            print("ðŸ¤– ä½¿ç”¨ OpenAI æ¨¡åž‹...")
            if not os.getenv("OPENAI_API_KEY"):
                raise ValueError("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° OPENAI_API_KEYã€‚è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®šã€‚")
            llm = ChatOpenAI(model=CHATGPT_MODEL_NAME, temperature=0.7, max_tokens=1024, streaming=True)
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        elif LLM_PROVIDER == "gemini":
            print("âœ¨ ä½¿ç”¨ Google Gemini æ¨¡åž‹...")
            if not os.getenv("GOOGLE_API_KEY"):
                raise ValueError("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GOOGLE_API_KEYã€‚è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®šã€‚")
            llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME, temperature=0.7, streaming=True)
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small") # ä»ç„¶å¯ä»¥ä½¿ç”¨ OpenAI çš„ embedding
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ LLM æä¾›è€…: {LLM_PROVIDER}ã€‚è«‹é¸æ“‡ 'openai' æˆ– 'gemini'ã€‚")
        return llm, embeddings

    def _load_or_create_vector_store(self):
        if os.path.exists(FAISS_INDEX_PATH):
            print(f"âœ… ç™¼ç¾å·²å»ºç«‹çš„å‘é‡è¨˜æ†¶åº«ï¼Œæ­£åœ¨å¾ž '{FAISS_INDEX_PATH}' è¼‰å…¥...")
            start_time = time.time()
            vector_store = FAISS.load_local(FAISS_INDEX_PATH, self.embeddings, allow_dangerous_deserialization=True)
            print(f"âš¡ è¨˜æ†¶åº«è¼‰å…¥å®Œæˆï¼Œè€—æ™‚ {time.time() - start_time:.2f} ç§’ã€‚")
            return vector_store
        else:
            print(f"ðŸ§  æœªç™¼ç¾ç¾æœ‰è¨˜æ†¶åº«ï¼Œæ­£åœ¨å¾ž '{JSON_DATA_PATH}' è®€å–ä¸¦å»ºç«‹æ–°çš„å‘é‡è¨˜æ†¶åº«...")
            try:
                with open(JSON_DATA_PATH, 'r', encoding='utf-8') as f:
                    qa_data = json.load(f)
            except FileNotFoundError:
                raise FileNotFoundError(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„ JSON æª”æ¡ˆ '{JSON_DATA_PATH}'ã€‚")
            except json.JSONDecodeError:
                raise ValueError(f"éŒ¯èª¤ï¼š'{JSON_DATA_PATH}' ä¸æ˜¯ä¸€å€‹æœ‰æ•ˆçš„ JSON æª”æ¡ˆã€‚")

            documents = []
            for item in qa_data:
                question = item.get('input') or item.get('instruction', 'æœªçŸ¥å•é¡Œ')
                answer = item.get('output', 'æœªçŸ¥å›žç­”')
                content = f"å•é¡Œï¼š{question.strip()}\nå›žç­”ï¼š{answer.strip()}"
                doc = Document(page_content=content, metadata={"source": JSON_DATA_PATH})
                documents.append(doc)
            
            print(f"ï¿½ å·²æˆåŠŸå°‡ {len(documents)} ç­† QA å°è½‰æ›ç‚ºç¨ç«‹çš„è¨˜æ†¶ç‰‡æ®µã€‚")
            print("ðŸ”® æ­£åœ¨å°‡è¨˜æ†¶å‘é‡åŒ–... (é¦–æ¬¡å»ºç«‹å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“)")
            start_time = time.time()
            vector_store = FAISS.from_documents(documents, self.embeddings)
            print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œè€—æ™‚ {time.time() - start_time:.2f} ç§’ã€‚")
            print(f"ðŸ’¾ æ­£åœ¨å°‡æ–°å»ºç«‹çš„è¨˜æ†¶åº«å„²å­˜è‡³ '{FAISS_INDEX_PATH}' ä»¥ä¾›æœªä¾†ä½¿ç”¨...")
            vector_store.save_local(FAISS_INDEX_PATH)
            return vector_store

    def _print_and_pass_through(self, data, prompt_name=""):
        if ENABLE_DEBUG_MODE:
            print("\n\n" + "="*25 + f" [DEBUG] Passing through '{prompt_name}' " + "="*25)
            if hasattr(data, 'to_string'):
                print(data.to_string())
            elif isinstance(data, dict):
                for key, value in data.items():
                    print(f"   - {key}: {str(value)[:300]}...")
            else:
                print(str(data)[:500])
            print("="*75 + "\n")
        return data

    def _create_rag_chain(self):
        if ENABLE_CONVERSATIONAL_MEMORY:
            print("ðŸ”— æ­£åœ¨çµ„è£å…·å‚™è¨˜æ†¶çš„ RAG è™•ç†éˆ (LCEL)...")
            
            contextualize_q_system_prompt = """çµ¦å®šä¸€æ®µå°è©±æ­·å²å’Œä¸€å€‹æœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œï¼Œé€™å€‹å•é¡Œå¯èƒ½å¼•ç”¨äº†å°è©±æ­·å²ä¸­çš„ä¸Šä¸‹æ–‡ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡é€™å€‹ä½¿ç”¨è€…å•é¡Œæ”¹å¯«æˆä¸€å€‹ç¨ç«‹çš„ã€ç„¡éœ€å°è©±æ­·å²å°±èƒ½ç†è§£çš„å•é¡Œã€‚ä¸è¦å›žç­”å•é¡Œï¼Œåªéœ€æ”¹å¯«å®ƒï¼Œå¦‚æžœå®ƒå·²ç¶“æ˜¯ç¨ç«‹å•é¡Œï¼Œå‰‡ç…§åŽŸæ¨£è¿”å›žã€‚"""
            contextualize_q_prompt = ChatPromptTemplate.from_messages(
                [("system", contextualize_q_system_prompt), MessagesPlaceholder(variable_name="chat_history"), ("human", "{input}")]
            )
            history_aware_retriever_chain = (
                contextualize_q_prompt
                | RunnableLambda(lambda p: self._print_and_pass_through(p, "Contextualize Question Prompt"))
                | self.llm 
                | StrOutputParser()
            )

            qa_prompt_template = self.cot_system_prompt if ENABLE_COT_MODE else self.system_prompt
            qa_prompt = ChatPromptTemplate.from_messages([
                ("system", qa_prompt_template),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", HUMAN_PROMPT_TEMPLATE),
            ])

            def format_docs(docs):
                return "\n\n".join(doc.page_content for doc in docs)

            question_answer_chain = (
                qa_prompt
                | RunnableLambda(lambda p: self._print_and_pass_through(p, "Final QA Prompt"))
                | self.llm
                | StrOutputParser()
            )
            
            rag_chain = (
                RunnablePassthrough.assign(
                    context=lambda x: (history_aware_retriever_chain | self.retriever | format_docs).invoke(x)
                )
                | question_answer_chain
            )
            return rag_chain
        else: 
            print("ðŸ”— æ­£åœ¨çµ„è£ç„¡ç‹€æ…‹çš„ RAG è™•ç†éˆ (LCEL)...")
            prompt = ChatPromptTemplate.from_messages([
                ("system", self.system_prompt),
                ("human", HUMAN_PROMPT_TEMPLATE)
            ])
            def format_docs(docs):
                return "\n\n".join(doc.page_content for doc in docs)
            rag_chain = (
                {"context": self.retriever | format_docs, "input": RunnablePassthrough()}
                | prompt
                | RunnableLambda(lambda p: self._print_and_pass_through(p, "Stateless QA Prompt"))
                | self.llm
                | StrOutputParser()
            )
            return rag_chain

    def _get_session_history(self, session_id: str) -> List[HumanMessage | AIMessage]:
        """æ ¹æ“š session_id ç²å–æˆ–å‰µå»ºå°è©±æ­·å²"""
        if session_id not in self.chat_histories:
            self.chat_histories[session_id] = []
        return self.chat_histories[session_id]

    def clear_history(self, session_id: str):
        """æ¸…ç†æŒ‡å®š session_id çš„å°è©±æ­·å²"""
        if session_id in self.chat_histories:
            del self.chat_histories[session_id]
            print(f"ðŸ§¹ å·²æ¸…é™¤ Session ID '{session_id}' çš„å°è©±æ­·å²ã€‚")

    async def get_response(self, user_input: str, session_id: str) -> str:
        """è™•ç†å–®æ¬¡è«‹æ±‚ä¸¦å›žå‚³å®Œæ•´ç­”æ¡ˆ"""
        if ENABLE_CONVERSATIONAL_MEMORY:
            chat_history = self._get_session_history(session_id)
            stream_input = {"input": user_input, "chat_history": chat_history}
            full_response = await self.rag_chain.ainvoke(stream_input)
            
            chat_history.append(HumanMessage(content=user_input))
            chat_history.append(AIMessage(content=full_response))
            # é™åˆ¶æ­·å²é•·åº¦
            if len(chat_history) > 10:
                self.chat_histories[session_id] = chat_history[-10:]
        else:
            full_response = await self.rag_chain.ainvoke(user_input)
            
        return full_response

    async def stream_response(self, user_input: str, session_id: str) -> AsyncGenerator[str, None]:
        """è™•ç†ä¸²æµè«‹æ±‚ä¸¦é€æ­¥å›žå‚³çµæžœ"""
        full_response = ""
        if ENABLE_CONVERSATIONAL_MEMORY:
            chat_history = self._get_session_history(session_id)
            stream_input = {"input": user_input, "chat_history": chat_history}
            
            async for chunk in self.rag_chain.astream(stream_input):
                full_response += chunk
                yield chunk
            
            chat_history.append(HumanMessage(content=user_input))
            chat_history.append(AIMessage(content=full_response))
            if len(chat_history) > 10:
                self.chat_histories[session_id] = chat_history[-10:]
        else:
            async for chunk in self.rag_chain.astream(user_input):
                yield chunk